{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Deployment Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the Machine Learning Model Deployment Project! This notebook serves as the foundation for the summative assignment where I design, build, and deploy a classification model. The goal is to create an end-to-end ML pipeline—starting from data preprocessing and model training to deployment on a cloud platform with scalability and user interaction features.\n",
    "\n",
    "In this project, I will:\n",
    "- Develop a classification model using a tabular dataset of teenage pregnancies in Rwanda.\n",
    "- Evaluate its performance using key metrics such as accuracy, precision, recall, and F1-score.\n",
    "- Construct a modular pipeline with Python functions for preprocessing, training, and prediction.\n",
    "- Deploy the pipeline as a dockerized web application on a cloud platform enabling features like real-time predictions, data uploads, and model retraining.\n",
    "- Test the deployed model’s scalability by simulating a flood of requests and analyzing latency and response times.\n",
    "\n",
    "NB: The dataset used is synthetic due to the lack of non-generic Rwandan data online. However, it was synthesised using Rwandan data online. Some of these features are age ranges of kids and the level of education they would be expected to have and economic classes which are referred to as Ubudehe Categories.\n",
    "\n",
    "This notebook will walk through the offline development and evaluation of the model, laying the groundwork for the full deployment process. The final solution will be a user-friendly, scalable application that demonstrates practical ML engineering skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: BUILDING AND SAVING A CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for the risk categories\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# Load the saved data\n",
    "with open('../data/train/X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open('../data/train/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open('../data/train/X_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "with open('../data/train/y_val.pkl', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open('../data/test/X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open('../data/test/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "# Encoding the risk categories\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# one-hot encoding for the risk categories\n",
    "y_train = tf.keras.utils.to_categorical(y_train_encoded, num_classes=3)\n",
    "y_val = tf.keras.utils.to_categorical(y_val_encoded, num_classes=3)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "def define_model(optimization: str, regularization_type: str = None, regularization_strength: float = 0.0, early_stopping: bool = False, learning_rate: float = False):\n",
    "    # Initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    def get_regularizer(reg_type, reg_strength):\n",
    "        if reg_type == 'l1':\n",
    "            return l1(reg_strength) if reg_strength > 0 else None\n",
    "        elif reg_type == 'l2':\n",
    "            return l2(reg_strength) if reg_strength > 0 else None\n",
    "        elif reg_type == 'l1_l2':\n",
    "            return l1_l2(l1=reg_strength, l2=reg_strength) if reg_strength > 0 else None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # First dense layer with optional L2 regularization\n",
    "    model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],),\n",
    "                    kernel_regularizer=get_regularizer(regularization_type, regularization_strength)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Second dense layer\n",
    "    model.add(Dense(16, activation='relu',\n",
    "                    kernel_regularizer=get_regularizer(regularization_type, regularization_strength)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Third dense layer\n",
    "    model.add(Dense(8, activation='relu',\n",
    "                    kernel_regularizer=get_regularizer(regularization_type, regularization_strength)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output layer with 3 neurons for multi-class classification\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimization,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Define callbacks for early stopping if required\n",
    "    callbacks = []\n",
    "    if early_stopping:\n",
    "        callbacks.append(EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True))\n",
    "\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing out the loss final model accuracy and ploting the loss curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def loss_curve_plot(history, y_test=None, y_pred=None):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    train_accuracy = history.history.get('accuracy', history.history.get('acc'))\n",
    "    val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history.history['loss'], 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Low Risk', 'Medium Risk', 'High Risk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model combinations with different optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Default model (No optimization techniques)\n",
    "\n",
    "default_model, _ = define_model(\n",
    "    optimization='adam',\n",
    "    regularization_type=None,\n",
    "    regularization_strength=0.0,\n",
    "    early_stopping=False,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "history = default_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(default_model.predict(X_test), axis=1)\n",
    "loss_curve_plot(history, y_test_labels, y_pred)\n",
    "\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nn_model_1, callbacks = define_model(\n",
    "    optimization='adam',\n",
    "    regularization_type='l1',\n",
    "    regularization_strength=0.05,\n",
    "    early_stopping=True,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "history = nn_model_1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(nn_model_1.predict(X_test), axis=1)\n",
    "loss_curve_plot(history, y_test_labels, y_pred)\n",
    "\n",
    "#confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy was higher than the training accuracy and this comes as a result of using the L1 Regularizer as it helps with generalising better on unseen data. However, the confusion matrix indicate that Low Risk samples were missclassified as High Risk classes. This confirms that the model doesn't work well with predicting Low Risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model 2\n",
    "\n",
    "nn_model_2, callbacks = define_model(\n",
    "    optimization='adam',\n",
    "    regularization_type='l2',\n",
    "    regularization_strength=0.09,\n",
    "    early_stopping=True,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "history = nn_model_2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(nn_model_2.predict(X_test), axis=1)\n",
    "loss_curve_plot(history, y_test_labels, y_pred)\n",
    "\n",
    "#confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of L2 regularization assures the prevention of overfitting since L2 regularization penalizes large weights more smoothly which encourages smaller weights overall. Also, results show a decrease in the loss compared to the previous model which indicates better perfomance of the model. However same as the previous model, the confusion matrix fails to predict low risk categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model 3\n",
    "\n",
    "nn_model_3, callbacks = define_model(\n",
    "    optimization='RMSProp',\n",
    "    regularization_type='l1',\n",
    "    regularization_strength=0.5,\n",
    "    early_stopping=True,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "history = nn_model_3.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(nn_model_3.predict(X_test), axis=1)\n",
    "loss_curve_plot(history, y_test_labels, y_pred)\n",
    "\n",
    "#confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RMSProp didn't help as much as shown from the results. Training and validation accuracies dropped significantly, suggesting underfitting, and the train and validation losses also increased which confirmed the underfitting. Also, the confusion matrix once again shows failure in predicting the low risk categories. We can conclude that RMSProp is the worst perfoming model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model 4\n",
    "\n",
    "nn_model_4, callbacks = define_model(\n",
    "    optimization='sgd',\n",
    "    regularization_type='l1_l2',\n",
    "    regularization_strength=0.1,\n",
    "    early_stopping=True,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "history = nn_model_4.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(nn_model_4.predict(X_test), axis=1)\n",
    "loss_curve_plot(history, y_test_labels, y_pred)\n",
    "\n",
    "#confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used SGD which updates weights using a fixed learning rate without momentum or adaptive learning rate adjustments, which can lead to avoid overfitting in some cases. With a combination of L1 and L2 without intensifying alot also contributed less underfitting. From the results, the train and validation accuracies indicate possible underfit of the training data but better generalization on the validation data. The issue of missclassifying low risk categories still persist which doesn't make this the best as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model 5\n",
    "\n",
    "nn_model_5, callbacks = define_model(\n",
    "    optimization='adam',\n",
    "    regularization_type='l1_l2',\n",
    "    regularization_strength=0.01,\n",
    "    early_stopping=True,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "history = nn_model_5.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(nn_model_5.predict(X_test), axis=1)\n",
    "loss_curve_plot(history, y_test_labels, y_pred)\n",
    "\n",
    "#confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "nn_model_5.save(\"../models/nn_model_5.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed back to adam optimization and this resulted into high training accuracy and validation accuracy indicating nice generalization and less overfitting due to the combination of adam and reduced strength of L1_L2 regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the confusion matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(\"../models/confusion_matrix.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "def svm_model(X_train, y_train, X_test, y_test):\n",
    "    # One-hot encoding: Convert y_train and y_test from one-hot to class labels\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Initialize and train the SVM model\n",
    "    model = SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Generate and print the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Low Risk', 'Medium Risk', 'High Risk']))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Call the function with your data\n",
    "svm_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth model is the best one so far so it was the one saved that i saved with its evaluation metrics and confusion table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesing the saved model\n",
    "\n",
    "import keras\n",
    "\n",
    "def make_predictions(model_path, X_test):\n",
    "    # Load the model\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(X_test)\n",
    "\n",
    "    # Convert probabilities to binary labels (0 or 1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return predictions, predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_path = '../models/nn_model_5.h5'\n",
    "predictions, predicted_classes = make_predictions(model_path, X_test)\n",
    "\n",
    "print(\"Raw Probabilities:\\n\", predictions)\n",
    "print(\"Predicted Classes:\\n\", predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python functions for the pipeline process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(X, fit_scaler=True, scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocess input data by scaling it.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data to preprocess.\n",
    "        fit_scaler (bool): Whether to fit a new scaler or use an existing one.\n",
    "        scaler (StandardScaler, optional): Pre-fitted scaler for transformation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Processed data and scaler object.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If input data is invalid or scaler is missing when required.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if X is a valid numpy array\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError(\"Input X must be a numpy array\")\n",
    "        \n",
    "        if fit_scaler:\n",
    "            # Create and fit a new scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_processed = scaler.fit_transform(X)\n",
    "        else:\n",
    "            # Use existing scaler\n",
    "            if scaler is None:\n",
    "                raise ValueError(\"Scaler must be provided when fit_scaler=False\")\n",
    "            X_processed = scaler.transform(X)\n",
    "        \n",
    "        return X_processed, scaler\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError in preprocess_data: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in preprocess_data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Model training/loading\n",
    "def get_model(model_path=None):\n",
    "    \"\"\"\n",
    "    Load an existing model or define a new one.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str, optional): Path to a saved model file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Model and callbacks (if new), or just the loaded model.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If model_path is invalid or file doesn't exist.\n",
    "        Exception: For other loading or model definition errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            return tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            # Assuming define_model is defined elsewhere\n",
    "            model, callbacks = define_model(\n",
    "                optimization='adam',\n",
    "                regularization_type='l1_l2',\n",
    "                regularization_strength=0.01,\n",
    "                early_stopping=True,\n",
    "                learning_rate=0.001\n",
    "            )\n",
    "            return model, callbacks\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Making predictions\n",
    "def make_predictions(model_path, X_test):\n",
    "    \"\"\"\n",
    "    Load a model and make predictions on test data.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model file.\n",
    "        X_test (np.ndarray): Test data for predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Raw probabilities and predicted classes.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If model file is missing.\n",
    "        ValueError: If X_test is invalid.\n",
    "        Exception: For prediction-related errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check inputs\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "        if not isinstance(X_test, np.ndarray):\n",
    "            raise ValueError(\"X_test must be a numpy array\")\n",
    "        \n",
    "        # Load model and predict\n",
    "        loaded_model = tf.keras.models.load_model(model_path)\n",
    "        predictions = loaded_model.predict(X_test)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return predictions, predicted_classes\n",
    "    \n",
    "    except FileNotFoundError as fe:\n",
    "        print(f\"FileNotFoundError in make_predictions: {fe}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError in make_predictions: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in make_predictions: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting the dataabse to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to database\n",
    "engine = create_engine('mongodb+srv://emunezero:qxSTjxixi1SbVmLf@summative.ret8ops.mongodb.net/')\n",
    "\n",
    "# Load data\n",
    "query = \"SELECT * FROM TeenPregnancyDB\"\n",
    "df = pd.read_sql(query, engine)\n",
    "password = 'qxSTjxixi1SbVmLf'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
